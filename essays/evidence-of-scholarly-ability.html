<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="title" content="Evidence of Scholarly Ability | Yang Qian | Professional Portfolio">
    <meta name="description" content="">
    <meta name="image" content="https://github.com/YangQiantwx.png">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Yang Qian | Professional Portfolio">
    <meta property="og:url" content="https://YangQiantwx.github.io/essays/evidence-of-scholarly-ability.html">
    <meta property="og:title" content="Evidence of Scholarly Ability | Yang Qian | Professional Portfolio">
    <meta property="og:description" content="">
    <meta property="og:image" content="https://github.com/YangQiantwx.png">
    <link rel="shortcut icon" href="/favicon.ico">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/techfolio-theme/default.css">
    <link rel="stylesheet" type="text/css" href="/css/rouge/github.css">
    <!-- Load MathJax if 'mathjax: true' is found in your _config.yml. -->
    
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
    </script>
    

    <title>Evidence of Scholarly Ability | Yang Qian | Professional Portfolio</title>
  </head>
  <body>
  <header class="navbar navbar-expand navbar-light bg-light bg-gradient border-bottom">
  <div class="container-fluid">
    <a class="navbar-brand" href="/">Yang Qian</a>
    <div class="ms-auto">
      <ul class="navbar-nav mb-2 mb-lg-0">
        <a class="nav-link" href="/#projects">Projects</a>
        <a class="nav-link" href="/#essays">Essays</a>
        <a class="nav-link" href="/resume.html">Resume</a>
      </ul>
    </div>
  </div>
</header>

<div class="container py-4">
  <h1 class="display-4">Evidence of Scholarly Ability</h1>
  <span class="date">01 Oct 2025</span>
  <hr>
  <h3 id="publications-and-scholarly-works">Publications and Scholarly Works</h3>

<p>Complete list available on <a href="https://scholar.google.com/citations?user=-LCbueIAAAAJ&amp;hl=en" target="_blank">Google Scholar</a>.</p>

<hr />

<h4 id="hashtag2action-data-engineering-and-self-supervised-pre-training-for-action-recognition-in-short-form-videos">Hashtag2Action: Data Engineering and Self-Supervised Pre-Training for Action Recognition in Short-Form Videos</h4>
<p><strong>Published in:</strong> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops — The First Workshop on Short-Form Video Understanding (SVU 2025)</em><br />
<strong>Authors:</strong> Yang Qian, Ali Kargarandehkordi, Yinan Sun, Parnian Azizian, Onur Cezmi Mutlu, Saimourya Surabhi, Zain Jabbar, Dennis Wall, Peter Washington, Huaijin Chen<br />
<strong>PDF:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/SVU/papers/Qian_Hashtag2Action_Data_Engineering_and_Self-Supervised_Pre-Training_for_Action_Recognition_in_ICCVW_2025_paper.pdf" target="_blank">View on CVF</a></p>

<p>Introduces the <strong>Hashtag2Action (H2A)</strong> pipeline that curates 283 k short-form video clips across 386 actions using adaptive hashtag mining and vision-based filtering for self-supervised VideoMAE V2 pre-training. Achieves competitive accuracy on UCF101, HMDB51, Kinetics-400, and SSv2 using only 20 % of the original pre-training data.</p>

<p><strong>Citation:</strong><br />
Qian Y, Kargarandehkordi A, Sun Y, Azizian P, Mutlu O C, Surabhi S, Jabbar Z, Wall D P, Washington P, Chen H. (2025). <em>Hashtag2Action: Data Engineering and Self-Supervised Pre-Training for Action Recognition in Short-Form Videos.</em> ICCV Workshops (SVU 2025), Honolulu, Hawai‘i.</p>

<hr />

<h4 id="a-linear-programming-and-deep-reinforcement-learning-framework-to-choose-dwell-positions-and-dwell-time-in-high-dose-rate-prostate-brachytherapy-using-curvilinear-catheters">A Linear Programming and Deep Reinforcement Learning Framework to Choose Dwell Positions and Dwell Time in High-Dose-Rate Prostate Brachytherapy Using Curvilinear Catheters</h4>
<p><strong>Published in:</strong> <em>ASME International Mechanical Engineering Congress and Exposition (IMECE 2024)</em><br />
<strong>Authors:</strong> Yang Qian, Peter Washington, Tarun K. Podder, Bardia Konh</p>

<p>Proposes a two-stage optimization framework combining linear programming and deep reinforcement learning (DDPG) to jointly select dwell positions and times for HDR prostate brachytherapy, achieving improved dosimetric balance and organ protection.</p>

<p><strong>Citation:</strong> Qian Y, Washington P, Podder T K, Konh B. (2024). <em>A Linear Programming and Deep Reinforcement Learning Framework to Choose Dwell Positions and Dwell Time in High-Dose-Rate Prostate Brachytherapy Using Curvilinear Catheters.</em> ASME IMECE 2024.</p>

<hr />

<h4 id="computer-vision-estimation-of-emotion-reaction-intensity-in-the-wild">Computer Vision Estimation of Emotion Reaction Intensity in the Wild</h4>
<p><strong>Published in:</strong> <em>arXiv preprint arXiv:2303.10741</em><br />
<strong>Authors:</strong> Yang Qian, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Mohammadmahdi Honarmand, Dennis P. Wall, Peter Washington<br />
<strong>PDF:</strong> <a href="https://arxiv.org/abs/2303.10741" target="_blank">View on arXiv</a></p>

<p>Develops vision-based and multimodal deep models to estimate continuous emotion reaction intensity for the Hume-Reaction dataset, achieving a Pearson correlation of 0.408 and advancing fine-grained affective computing beyond discrete emotion labels.</p>

<p><strong>Citation:</strong> Qian Y, Kargarandehkordi A, Mutlu O C, Surabhi S, Honarmand M, Wall D P, Washington P. (2023). <em>Computer Vision Estimation of Emotion Reaction Intensity in the Wild.</em> arXiv preprint arXiv:2303.10741.</p>

<hr />

<h4 id="enhancing-automatic-emotion-recognition-for-clinical-applications">Enhancing Automatic Emotion Recognition for Clinical Applications</h4>
<p><strong>Published by:</strong> <em>University of Hawai‘i at Mānoa (MS Thesis)</em><br />
<strong>Author:</strong> Yang Qian (2023)</p>

<p>Master’s thesis investigating personalized multimodal transformer architectures for automatic emotion recognition and reaction intensity quantification in clinical settings. Approved as Plan A thesis for the M.S. in Computer Science program.</p>

</div>

<footer class="navbar navbar-expand navbar-light bg-light bg-gradient border-top">
  <div class="container-fluid">
    <div class="ms-auto">
      <ul class="navbar-nav mb-2 mb-lg-0">
        <small><a class="nav-link" href="https://techfolios.github.io">Made with Techfolios</a></small>
      </ul>
    </div>
  </div>
</footer>


  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js" integrity="sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2" crossorigin="anonymous"></script>
  </body>
</html>
